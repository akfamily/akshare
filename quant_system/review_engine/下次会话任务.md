# ä¸‹æ¬¡ä¼šè¯ä»»åŠ¡æ¸…å•

## ğŸ“Š å½“å‰è¿›åº¦ï¼ˆå·²å®Œæˆ30%ï¼‰

### âœ… ç¬¬ä¸€é˜¶æ®µï¼šæ¶æ„ä¸æ•°æ®å±‚ï¼ˆå·²å®Œæˆï¼‰

1. **é¡¹ç›®æ¶æ„** âœ…
   - å®Œæ•´çš„ç›®å½•ç»“æ„
   - æ¨¡å—åŒ–è®¾è®¡ï¼ˆproviders/core/reporting/integrationsï¼‰

2. **é…ç½®ç³»ç»Ÿ** âœ…
   - config.yamlï¼ˆæƒé‡ã€è§„åˆ™ã€ETFåˆ†ç»„ã€è¡Œä¸šè½®åŠ¨é…ç½®ï¼‰
   - .env.exampleï¼ˆç¯å¢ƒå˜é‡ï¼‰
   - å®Œæ•´çš„é…ç½®æ–‡æ¡£

3. **æ•°æ®å±‚åŸºç¡€** âœ…
   - AkShareProviderç±»æ¡†æ¶
   - SQLiteæ•°æ®åº“è®¾è®¡ï¼ˆ9å¼ è¡¨ï¼‰
   - æ•°æ®æ ¡éªŒæœºåˆ¶
   - é”™è¯¯å¤„ç†ä¸é‡è¯•
   - å·²å®ç°æ¥å£ï¼š
     - âœ… æŒ‡æ•°æ•°æ®ï¼ˆä¸Šè¯/æ·±æˆ/åˆ›ä¸šæ¿/æ²ªæ·±300ï¼‰
     - âœ… æˆäº¤é¢
     - âœ… å¸‚åœºå¹¿åº¦ï¼ˆæ¶¨è·Œå®¶æ•°ã€æ¶¨è·Œåœï¼‰
     - âœ… åŒ—å‘èµ„é‡‘

### ğŸš§ ç¬¬äºŒé˜¶æ®µï¼šå®Œå–„æ•°æ®æ¥å£ï¼ˆä¸‹æ¬¡ä¼šè¯ä¼˜å…ˆï¼‰

**ç›®æ ‡**ï¼šå®Œæˆæ‰€æœ‰AkShareæ•°æ®æ¥å£ï¼Œç¡®ä¿é›¶è™šæ‹Ÿæ•°æ®

#### 1. ETFæµå‘ï¼ˆé‡è¦ï¼ï¼‰
```python
def fetch_etf_flows(self, date: str, bucket: str) -> Dict:
    """
    è·å–ETFåˆ†ç»„çš„å‡€æµå‘

    å®ç°æ–¹å¼ï¼š
    1. ä½¿ç”¨ ak.fund_etf_fund_info_em() è·å–ä»½é¢æ•°æ®
    2. è®¡ç®—ä»½é¢å˜åŒ–ä½œä¸ºå‡€æµå‘ä»£ç†
    3. æŒ‰config.yamlä¸­çš„åˆ†ç»„èšåˆ

    åˆ†ç»„é…ç½®ï¼š
    - broad: ["510300", "510500", "159915"]
    - growth: ["159949", "159915"]
    - value: ["510880", "510900"]
    - industry_resources: ["510170", "159980"]
    - industry_tech: ["159801", "159841"]
    """
    pass
```

#### 2. èèµ„èåˆ¸ä½™é¢
```python
def fetch_margin(self, date: str) -> Dict:
    """
    è·å–ä¸¤å¸‚èèµ„èåˆ¸ä½™é¢

    å®ç°æ–¹å¼ï¼š
    1. ak.stock_margin_sse() æ²ªå¸‚
    2. ak.stock_margin_szse() æ·±å¸‚
    3. åˆå¹¶è®¡ç®—æ€»ä½™é¢
    """
    pass
```

#### 3. è¡Œä¸šæ•°æ®èšåˆ
```python
def fetch_industry_data(self, date: str) -> pd.DataFrame:
    """
    èšåˆè¡Œä¸šçº§åˆ«æ•°æ®

    æ•°æ®æ¥æºï¼š
    1. ä» ak.stock_zh_a_spot_em() è·å–ä¸ªè‚¡æ•°æ®
    2. æŒ‰"è¡Œä¸š"å­—æ®µåˆ†ç»„
    3. è®¡ç®—ï¼š
       - avg_return: è¡Œä¸šå¹³å‡æ¶¨è·Œå¹…
       - net_flow: è¡Œä¸šä¸»åŠ›å‡€æµå…¥åˆè®¡
       - avg_turnover: è¡Œä¸šå¹³å‡æ¢æ‰‹ç‡
       - limit_up_count: æ¶¨åœå®¶æ•°ï¼ˆä»æ¶¨åœæ± åˆ†ç»„ï¼‰

    æ³¨æ„ï¼šéœ€è¦å¤„ç†è¡Œä¸šå­—æ®µçš„å¤šç§åç§°
    """
    pass
```

#### 4. å®è§‚æ•°æ®
```python
def fetch_macro_data(self, date: str) -> Dict:
    """
    è·å–å®è§‚ä»£ç†æŒ‡æ ‡

    æ•°æ®æºï¼š
    1. PMI: ak.macro_china_pmi()
       - å–"æ–°è®¢å•æŒ‡æ•°"
       - å–"äº§æˆå“åº“å­˜"

    2. PPI: ak.macro_china_ppi()
       - å–åŒæ¯”ï¼ˆè‹¥æ— ç¯æ¯”ï¼‰

    3. å¤§å®—å•†å“: ak.macro_china_commodity_index()
       - æˆ–è‡ªå®šä¹‰å¤§å®—ç¯®å­æ”¶ç›Š
    """
    pass
```

#### 5. ParquetæŒä¹…åŒ–
```python
def save_to_parquet(self, table_name: str, df: pd.DataFrame):
    """
    ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®åˆ°Parquet

    ä¼˜åŠ¿ï¼š
    - å¿«é€Ÿè¯»å†™
    - æ”¯æŒå›æµ‹
    - åˆ—å¼å­˜å‚¨èŠ‚çœç©ºé—´
    """
    pass
```

#### 6. è¿æ¿æŒç»­ç‡ï¼ˆéœ€å¤šæ—¥æ•°æ®ï¼‰
```python
def calculate_continuous_limit_rate(self, date: str, lookback: int = 20) -> float:
    """
    è®¡ç®—è¿æ¿æŒç»­ç‡

    é€»è¾‘ï¼š
    1. ä»SQLiteè¯»å–è¿‡å»lookbackæ—¥çš„æ¶¨åœæ± æ•°æ®
    2. ç»Ÿè®¡ï¼šæ˜¨æ—¥è¿æ¿æ ‡çš„ â†’ ä»Šæ—¥ä¸Šæ¶¨çš„æ¯”ä¾‹
    3. è¿”å›æŒç»­ç‡

    æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦å†å²æ•°æ®ç§¯ç´¯
    """
    pass
```

---

## ğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šæ ¸å¿ƒè®¡ç®—æ¨¡å—

### 1. å› å­è®¡ç®—ï¼ˆcore/factors.pyï¼‰

**ç›®æ ‡**ï¼šå®ç°å¤šå‘¨æœŸè¶‹åŠ¿å› å­

```python
"""
å› å­è®¡ç®—æ¨¡å—

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å¤šå‘¨æœŸEMAï¼ˆ5/10/30ï¼‰
2. çº¿æ€§æ–œç‡ï¼ˆå›å½’Î²ï¼‰
3. rolling z-score
4. è¶‹åŠ¿æ ‡ç­¾ç”Ÿæˆ
"""

import pandas as pd
import numpy as np
from typing import Dict, List
from scipy import stats

def compute_multi_horizon(series: pd.Series, windows: List[int] = [5, 10, 30]) -> Dict:
    """
    è®¡ç®—å¤šå‘¨æœŸè¶‹åŠ¿å› å­

    Args:
        series: æ—¶é—´åºåˆ—ï¼ˆå¦‚æˆäº¤é¢ã€åŒ—å‘èµ„é‡‘ï¼‰
        windows: çª—å£åˆ—è¡¨

    Returns:
        {
            5: {ema: float, slope: float, z: float},
            10: {...},
            30: {...}
        }
    """
    results = {}

    for w in windows:
        # 1. EMA
        ema = series.ewm(span=w).mean().iloc[-1]

        # 2. çº¿æ€§å›å½’æ–œç‡
        x = np.arange(w)
        y = series.iloc[-w:].values
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)

        # 3. z-scoreï¼ˆåŸºäºæ›´é•¿çª—å£max(10, w)ï¼‰
        z_window = max(10, w)
        mean = series.iloc[-z_window:].mean()
        std = series.iloc[-z_window:].std()
        z_score = (series.iloc[-1] - mean) / std if std > 0 else 0

        results[w] = {
            'ema': ema,
            'slope': slope,
            'z': z_score
        }

    return results

def trend_label(trends_dict: Dict) -> str:
    """
    ç”Ÿæˆè¶‹åŠ¿æ ‡ç­¾

    Args:
        trends_dict: compute_multi_horizonçš„è¾“å‡º

    Returns:
        è¶‹åŠ¿æè¿°ï¼ˆå¦‚"å¤šå‘¨æœŸå…±æŒ¯ä¸Šè¡Œ"ï¼‰
    """
    slopes = [v['slope'] for v in trends_dict.values()]

    # å¤šå‘¨æœŸå…±æŒ¯åˆ¤æ–­
    all_up = all(s > 0 for s in slopes)
    all_down = all(s < 0 for s in slopes)

    if all_up:
        return "å¤šå‘¨æœŸå…±æŒ¯ä¸Šè¡Œ"
    elif all_down:
        return "å¤šå‘¨æœŸå…±æŒ¯èµ°å¼±"
    elif slopes[0] > 0 and slopes[1] > 0:
        return "çŸ­ä¸­ä¸Šè¡Œã€é•¿æœªä¿®å¤"
    elif slopes[0] > 0:
        return "çŸ­ä¿®å¤ã€é•¿åå¼±"
    else:
        return "åˆ†åŒ–éœ‡è¡"
```

---

### 2. è¯„åˆ†ç³»ç»Ÿï¼ˆcore/scoring.pyï¼‰

**ç›®æ ‡**ï¼šå››ç»´è¯„åˆ† â†’ æ€»åˆ†

```python
"""
è¯„åˆ†ç³»ç»Ÿ

å››ç»´è¯„åˆ†ï¼š
1. Macroï¼ˆ25%ï¼‰
2. Liquidityï¼ˆ35%ï¼‰
3. Risk-onï¼ˆ20%ï¼‰
4. Momentumï¼ˆ20%ï¼‰
"""

def score_liquidity(trends: Dict, config: dict) -> float:
    """
    æµåŠ¨æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰

    è¾“å…¥ï¼š
    - æˆäº¤é¢å¤šå‘¨æœŸè¶‹åŠ¿
    - åŒ—å‘èµ„é‡‘å¤šå‘¨æœŸè¶‹åŠ¿
    - ETFåˆ†ç»„æµå‘
    - èèµ„ä½™é¢è¶‹åŠ¿

    é€»è¾‘ï¼š
    1. å¯¹æ¯ä¸ªåºåˆ—çš„slopeå’Œz-scoreå½’ä¸€åŒ–åˆ°0-100
    2. æŒ‰æƒé‡åŠ æƒ
    3. è¿”å›ç»¼åˆå¾—åˆ†
    """
    pass

def score_riskon(breadth_data: Dict, config: dict) -> float:
    """
    Risk-onè¯„åˆ†ï¼ˆ0-100ï¼‰

    è¾“å…¥ï¼š
    - è¿æ¿é«˜åº¦
    - æ¶¨åœ/è·Œåœæ¯”
    - è¿æ¿æŒç»­ç‡

    é€»è¾‘ï¼š
    1. è®¡ç®—å„æŒ‡æ ‡çš„z-score
    2. å½’ä¸€åŒ–åˆ°0-100
    3. åŠ æƒåˆæˆ
    """
    pass

def score_momentum(indices_trends: Dict, amount_trends: Dict) -> float:
    """
    åŠ¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰

    è¾“å…¥ï¼š
    - å®½åŸºæŒ‡æ•°åŠ¨é‡ï¼ˆ5/10/30æ–œç‡ï¼‰
    - æˆäº¤é¢è¶‹åŠ¿
    """
    pass

def score_macro(pmi: Dict, ppi: float, commodity: float) -> float:
    """
    å®è§‚è¯„åˆ†ï¼ˆ0-100ï¼‰

    è¾“å…¥ï¼š
    - PMIæ–°è®¢å•/åº“å­˜ â†’ åŸºé’¦å‘¨æœŸå››è±¡é™
    - PPI
    - å¤§å®—å•†å“æ”¶ç›Š

    é€»è¾‘ï¼š
    1. å››è±¡é™æ˜ å°„åˆ°è¯„åˆ†
    2. ç»“åˆPPIå’Œå¤§å®—å•†å“è°ƒèŠ‚
    """
    pass

def total_score(scores: Dict, weights: Dict) -> float:
    """
    åˆæˆæ€»åˆ†

    Args:
        scores: {macro: xx, liquidity: xx, riskon: xx, momentum: xx}
        weights: {macro: 0.25, liquidity: 0.35, ...}

    Returns:
        æ€»åˆ†ï¼ˆ0-100ï¼‰
    """
    return sum(scores[k] * weights[k] for k in scores)
```

---

### 3. è¡Œä¸šè½®åŠ¨ä¸é…ç½®ï¼ˆcore/allocation.pyï¼‰

```python
"""
èµ„äº§é…ç½®æ¨¡å—

åŒ…å«ï¼š
1. è¡Œä¸šè½®åŠ¨çŸ©é˜µ
2. ä»“ä½/é£æ ¼å†³ç­–
3. è¡Œä¸šè¶…/ä½é…é€‰æ‹©
"""

def rotation_map(industry_df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    è¡Œä¸šè½®åŠ¨çŸ©é˜µ

    è¾“å…¥ï¼š
    - industry_df: è¡Œä¸šæ•°æ®ï¼ˆavg_return, net_flow, avg_turnover, limit_up_countï¼‰

    è¾“å‡ºï¼š
    - DataFrame: [industry, strength, crowding, quadrant, recommendation]

    é€»è¾‘ï¼š
    1. å¼ºåº¦ = 0.6 * return_percentile + 0.4 * net_flow_percentile
    2. æ‹¥æŒ¤åº¦ = 0.6 * turnover_percentile + 0.4 * limit_up_percentile
    3. å››è±¡é™åˆ†ç±»
    4. ç”Ÿæˆå»ºè®®ï¼ˆè¶…é…/æ ‡é…/ä½é…ï¼‰
    """
    pass

def decide_allocation(total_score: float, config: dict) -> Dict:
    """
    å†³å®šä»“ä½å’Œé£æ ¼

    è¾“å…¥ï¼š
    - total_score: æ€»åˆ†ï¼ˆ0-100ï¼‰
    - config: allocation_rules

    è¾“å‡ºï¼š
    - {position: 65, style: "å‡è¡¡/è½»åé¡ºå‘¨æœŸ"}
    """
    for rule in config['allocation_rules']:
        if rule['score_min'] <= total_score < rule['score_max']:
            # çº¿æ€§æ’å€¼è®¡ç®—ä»“ä½
            ratio = (total_score - rule['score_min']) / (rule['score_max'] - rule['score_min'])
            position = rule['position_min'] + ratio * (rule['position_max'] - rule['position_min'])

            return {
                'position': int(position),
                'style': rule['style']
            }

    return {'position': 50, 'style': 'å‡è¡¡'}

def pick_industries(rotation_df: pd.DataFrame) -> Dict:
    """
    é€‰æ‹©è¶…é…/ä½é…è¡Œä¸š

    é€»è¾‘ï¼š
    1. è¶…é…ï¼šquadrant="é¦–é€‰" ä¸” crowding<0.7 çš„Top3
    2. ä½é…ï¼šquadrant="å›é¿" çš„Top3
    3. å…¶ä½™ä¸ºæ ‡é…
    """
    pass
```

---

### 4. æŠ¥å‘Šç”Ÿæˆï¼ˆreporting/renderer.pyï¼‰

```python
"""
æŠ¥å‘Šç”Ÿæˆå™¨

è¾“å‡ºï¼š
1. Markdownæ–‡æœ¬æŠ¥å‘Š
2. JSONç»“æ„åŒ–æ•°æ®
3. å›¾è¡¨ï¼ˆå¯é€‰ï¼‰
"""

def generate_markdown(data: Dict) -> str:
    """
    ç”ŸæˆMarkdownæŠ¥å‘Š

    ç»“æ„ï¼š
    # ä»Šæ—¥å¸‚åœºå¤ç›˜ï¼ˆYYYY-MM-DDï¼‰

    ## æ‘˜è¦
    - æ€»åˆ†ï¼šXX
    - ä»“ä½ï¼šXX%
    - é£æ ¼ï¼šXX

    ## é‡èƒ½ä¸èµ„é‡‘
    ### æˆäº¤é¢
    - çŸ­æœŸï¼ˆ5æ—¥ï¼‰ï¼šè¶‹åŠ¿æ ‡ç­¾ï¼ŒEMA=XXï¼Œæ–œç‡=XX
    - ...

    ### åŒ—å‘èµ„é‡‘
    - ...

    ## å››ç»´è¯„åˆ†
    | ç»´åº¦ | å¾—åˆ† | æƒé‡ |
    |------|------|------|
    | Macro | XX | 25% |
    | ...

    ## è¡Œä¸šè½®åŠ¨
    | è¡Œä¸š | å¼ºåº¦ | æ‹¥æŒ¤åº¦ | è±¡é™ | å»ºè®® |
    | ... |

    ## ä¸‰æ®µå¼æ”¶æŸ
    ### åˆ©å¥½
    - ...

    ### åˆ©ç©º
    - ...

    ### ç»“è®º
    ä»“ä½XX%ï¼Œæ“ä½œXXï¼Œé£æ ¼XXï¼Œè¡Œä¸šï¼ˆè¶…é…XXï¼Œä½é…XXï¼‰ï¼Œé£é™©æç¤ºXX
    """
    pass

def generate_json(data: Dict) -> Dict:
    """
    ç”Ÿæˆç»“æ„åŒ–JSON

    æ ¼å¼è§PRDç¬¬8èŠ‚
    """
    pass
```

---

### 5. ä¸»ç¨‹åºï¼ˆrun_daily.pyï¼‰

```python
"""
ä¸»ç¨‹åº

è¿è¡Œå®Œæ•´çš„æ—¥åº¦å¤ç›˜æµç¨‹
"""

import yaml
from datetime import datetime
from providers.akshare_provider import AkShareProvider
from core.factors import compute_multi_horizon, trend_label
from core.scoring import *
from core.allocation import *
from reporting.renderer import *

def run_daily_review(date: str = None, config_path: str = 'config/config.yaml'):
    """
    è¿è¡Œæ—¥åº¦å¤ç›˜

    Args:
        date: æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä»Šæ—¥
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    """
    # 1. åŠ è½½é…ç½®
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # 2. åˆå§‹åŒ–æ•°æ®æä¾›è€…
    provider = AkShareProvider(config)

    # 3. æ‹‰å–æ•°æ®
    provider.fetch_and_save_all(date)

    # 4. è®¡ç®—å› å­
    # ...

    # 5. è®¡ç®—è¯„åˆ†
    # ...

    # 6. å†³å®šé…ç½®
    # ...

    # 7. ç”ŸæˆæŠ¥å‘Š
    # ...

    # 8. è¾“å‡º
    print(markdown_report)
    with open(f'output/review_{date}.json', 'w') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--date', default=None)
    parser.add_argument('--export', default='output/')
    args = parser.parse_args()

    run_daily_review(args.date)
```

---

## ğŸ¯ ä¸‹æ¬¡ä¼šè¯ç›®æ ‡

### å¿…é¡»å®Œæˆï¼š
1. âœ… å®Œå–„æ•°æ®æä¾›è€…ï¼ˆETFã€èèµ„èåˆ¸ã€è¡Œä¸šã€å®è§‚ã€Parquetï¼‰
2. âœ… å®ç°å› å­è®¡ç®—æ¨¡å—
3. âœ… å®ç°è¯„åˆ†ç³»ç»Ÿ
4. âœ… å®ç°é…ç½®å†³ç­–
5. âœ… å®ç°æŠ¥å‘Šç”Ÿæˆ
6. âœ… åˆ›å»ºä¸»ç¨‹åº
7. âœ… ç¼–å†™éªŒæ”¶æµ‹è¯•

### å¯é€‰å®Œæˆï¼š
- DeepSeekæ¥å£ï¼ˆå¸¦å¼€å…³ï¼‰
- å›¾è¡¨ç”Ÿæˆ
- Webç•Œé¢é›†æˆ

---

## ğŸ“‹ éªŒæ”¶æ ‡å‡†ï¼ˆå¿…é¡»å…¨è¿‡ï¼‰

1. **é›¶æ¨¡æ‹Ÿæ•°æ®**ï¼šä»£ç ä¸­ä¸å¾—å‡ºç°`random`ã€`np.random`ç­‰
2. **æ•°æ®å®Œæ•´æ€§**ï¼šæ‰€æœ‰è¡¨éƒ½æœ‰æ•°æ®ï¼Œæ— å…¨NaNåˆ—
3. **å¤šå‘¨æœŸ**ï¼šè¾“å‡ºJSONåŒ…å«5/10/30çš„ema/slope/z
4. **è¯„åˆ†**ï¼šå››ç»´+æ€»åˆ†å…¨åœ¨0-100
5. **é…ç½®**ï¼šä»“ä½åœ¨20-90%
6. **æŠ¥å‘Š**ï¼šåŒ…å«åˆ©å¥½/åˆ©ç©º/ç»“è®ºä¸‰æ®µ
7. **æŒä¹…åŒ–**ï¼šSQLiteæœ‰9å¼ è¡¨ï¼ŒParquetç›®å½•éç©º
8. **é”™è¯¯å¤„ç†**ï¼šå¤±è´¥æ—¶æœ‰æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯

---

**å½“å‰ç‰ˆæœ¬**ï¼š0.1.0ï¼ˆæ¶æ„ä¸æ•°æ®å±‚ï¼‰
**ä¸‹æ¬¡ç›®æ ‡**ï¼š0.5.0ï¼ˆMVPå¯è¿è¡Œç‰ˆæœ¬ï¼‰
**æœ€ç»ˆç›®æ ‡**ï¼š1.0.0ï¼ˆå®Œæ•´PRDå®ç°ï¼‰
